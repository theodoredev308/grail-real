# ============================================================================
# NETWORK SELECTION
# ============================================================================
# Which Bittensor network to connect to (if BT_CHAIN_ENDPOINT is not set):
# - finney : mainnet (default) - SDK resolves to official mainnet endpoint
# - test   : testnet - SDK resolves to official testnet endpoint  
# - local  : local node (typically ws://localhost:9944)
BT_NETWORK=finney

# Optional: Override with explicit websocket endpoint for custom/local nodes
# If set, this takes precedence over BT_NETWORK
# Examples:
#   - Local node: ws://localhost:9944
#   - Custom remote: wss://my-node.example.com:443
# Leave empty to use BT_NETWORK (recommended for production)
BT_CHAIN_ENDPOINT=

# Your target subnet NETUID (e.g., 81 mainnet, 429 your test subnet)
NETUID=81

# ============================================================================
# WALLET CONFIGURATION
# ============================================================================
# These are *names* (aliases) you assign when creating the coldkey and hotkey.
# You can create a new one for mining/validating like `btcli wallet new_coldkey --wallet.name mywallet`
# Not the SS58 address or file path. Just the user-chosen wallet name.
# 1. BT_WALLET_COLD — the name you gave your coldkey (e.g., "mywallet")
# This key stays offline and is used for staking, registration, and ownership.
BT_WALLET_COLD=

# 2. BT_WALLET_HOT — the name you gave your hotkey (e.g., "myhotkey")
# You can create a new one `btcli wallet new_hotkey --wallet.name mywallet --wallet.hotkey myhotkey`
# Note that mywallet here is the same from the previous set.
# This key is safe to use online and used for mining, validation, or inference.
BT_WALLET_HOT=default

# ============================================================================
# REQUIRED: STORAGE (Cloudflare R2) - DUAL CREDENTIAL SYSTEM
# ============================================================================
# IMPORTANT: GRAIL uses a dual-credential system for R2:
# - WRITE credentials: Private, never shared, used for uploading your data
# - READ credentials: Shared on-chain, allows others to read your data

# 1. R2_ACCOUNT_ID: Your Cloudflare account ID (used in API endpoints)
# ➤ Go to: https://dash.cloudflare.com > Click your account (top left) > Overview
# ➤ Copy the "Account ID" shown there.
R2_ACCOUNT_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# 2. R2_BUCKET_ID: The name of the R2 bucket you create
# ⚠️  IMPORTANT: The bucket name MUST be the same as your R2_ACCOUNT_ID above!
# ➤ Go to https://dash.cloudflare.com > R2 > "Create Bucket"
# ➤ Set bucket name to your Account ID (same value as R2_ACCOUNT_ID)
# ➤ Set region to ENAM (required)
# ➤ Example: If your account ID is "abc123def456", your bucket name should be "abc123def456"
R2_BUCKET_ID=""

# 3. WRITE CREDENTIALS (Private - NEVER shared on chain)
# These are your API credentials with full read/write access to R2
# ➤ Go to: https://dash.cloudflare.com > R2 > "Manage R2 API Tokens"
# ➤ Click "Create API Token"
# ➤ Name it something like "grail-write-access"
# ➤ Select **Edit Permissions**, and:
#   - Scope: `Account.Cloudflare R2 Storage` (or select R2 bucket explicitly)
#   - Permissions: `Edit` (for full read/write access)
# ➤ Generate and copy both keys.
R2_WRITE_ACCESS_KEY_ID=AKIAXXXXXXXXXXXXXXXX
R2_WRITE_SECRET_ACCESS_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# 4. READ CREDENTIALS (Public - Will be posted to chain)
# These are API credentials with read-only access to R2
# ➤ Go to: https://dash.cloudflare.com > R2 > "Manage R2 API Tokens"
# ➤ Click "Create API Token" again
# ➤ Name it something like "grail-read-only"
# ➤ Select **Read Permissions**, and:
#   - Scope: `Account.Cloudflare R2 Storage` (or select R2 bucket explicitly)
#   - Permissions: `Read` (for read-only access)
# ➤ Generate and copy both keys.
# NOTE: These credentials will be committed to the blockchain for transparency
R2_READ_ACCESS_KEY_ID=AKIAXXXXXXXXXXXXXXXX
R2_READ_SECRET_ACCESS_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# ============================================================================
# OPTIONAL: HUGGING FACE
# ============================================================================

# 5. HF_TOKEN: Your Hugging Face access token for uploading datasets
# ➤ Go to: https://huggingface.co/settings/tokens
# ➤ Click "New token" and create a token with write permissions
# ➤ Copy the token (starts with hf_...)
# This is used to upload validated rollouts to the public Hugging Face dataset
HF_TOKEN=hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# 6. HF_USERNAME: Your Hugging Face username (optional)
# ➤ This is your HuggingFace username (e.g., "fatheroffire")
# ➤ The dataset will be created at: {HF_USERNAME}/grail-sat-rollouts
# ➤ If not set, defaults to "fatheroffire"
HF_USERNAME=miahi

# ============================================================================
# GRAIL TRAINER MODEL LOADING CONFIGURATION
# ============================================================================
# The trainer supports flexible model loading at startup via environment variables.
# Both training and reference models can be loaded from different sources:
# - "latest": Download and use the latest checkpoint from R2 bucket
# - "hf": Load a model directly from Hugging Face Hub
# - "window": Load a specific checkpoint from a particular window

# Training Model Configuration
# Mode for loading the training model (required, no default)
# Options: latest | hf | window
GRAIL_TRAIN_MODEL_MODE=

# Required if GRAIL_TRAIN_MODEL_MODE=hf
# HuggingFace model ID (e.g., "Qwen/Qwen2.5-7B")
GRAIL_TRAIN_MODEL_ID=

# Required if GRAIL_TRAIN_MODEL_MODE=window
# Specific window number to load checkpoint from (e.g., 72000)
GRAIL_TRAIN_CHECKPOINT_WINDOW=

# Reference Model Configuration
# Mode for loading the reference model (required, no default)
# Options: latest | hf | window
GRAIL_REF_MODEL_MODE=

# Required if GRAIL_REF_MODEL_MODE=hf
# HuggingFace model ID (e.g., "Qwen/Qwen2.5-7B")
GRAIL_REF_MODEL_ID=

# Required if GRAIL_REF_MODEL_MODE=window
# Specific window number to load checkpoint from (e.g., 72000)
GRAIL_REF_CHECKPOINT_WINDOW=

# Example Configurations:
# 
# 1. Train with latest checkpoint, reference with HF model:
#    GRAIL_TRAIN_MODEL_MODE=latest
#    GRAIL_REF_MODEL_MODE=hf
#    GRAIL_REF_MODEL_ID=Qwen/Qwen2.5-7B
#
# 2. Train with HF model, reference with specific checkpoint:
#    GRAIL_TRAIN_MODEL_MODE=hf
#    GRAIL_TRAIN_MODEL_ID=Qwen/Qwen2.5-7B
#    GRAIL_REF_MODEL_MODE=window
#    GRAIL_REF_CHECKPOINT_WINDOW=72000
#
# 3. Both from latest checkpoints:
#    GRAIL_TRAIN_MODEL_MODE=latest
#    GRAIL_REF_MODEL_MODE=latest

# ============================================================================
# GRAIL MINER GENERATION CONFIGURATION
# ============================================================================
# Number of rollouts to generate in parallel per batch (default: 1)
# Higher values increase GPU utilization and throughput but require more VRAM.
# Must be <= ROLLOUTS_PER_PROBLEM (currently 16) and must divide evenly into it.
# Valid options: 1, 2, 4, 8, 16 (factors of ROLLOUTS_PER_PROBLEM)
# Recommended tuning: Start with 1, gradually increase to 2, 4, 8, or 16
# Monitor GPU memory with nvidia-smi to avoid OOM errors.
# Example values:
#   - 1: Sequential generation (baseline, lowest memory)
#   - 2: ~1.5-1.8x throughput
#   - 4: ~3-4x throughput
#   - 8: ~6-7x throughput
#   - 16: ~8-10x throughput (requires significant VRAM, ideal for H100/H200 144GB or B200)
GRAIL_GENERATION_BATCH_SIZE=4

# ============================================================================
# GRAIL TRAINING CONFIGURATION
# ============================================================================
# Learning rate for GRPO training (default: 2e-6)
GRAIL_TRAINER_LR=1e-6

# Number of training epochs per window (default: 2)
GRAIL_TRAINER_EPOCHS=2

# Batch size for training (default: 4)
GRAIL_TRAINER_BATCH_SIZE=4

# Maximum sequence length for training (default: 1024)
GRAIL_TRAINER_MAX_LENGTH=512

# Gradient clipping threshold (default: 0.5)
GRAIL_TRAINER_GRAD_CLIP=0.5

# Number of warmup steps for learning rate scheduler (default: 10)
GRAIL_TRAINER_WARMUP_STEPS=10

# KL divergence coefficient for regularization (default: 0.02)
GRAIL_TRAINER_KL_COEF=0.02

# Entropy coefficient for exploration (default: 0.001)
GRAIL_TRAINER_ENTROPY_COEF=0.001

# Advantage clipping percentile (default: 99.0)
GRAIL_TRAINER_ADV_CLIP_PERCENTILE=99.0

# Group advantage sum tolerance for GRPO (default: 0.01)
GRAIL_TRAINER_GROUP_ADV_SUM_TOL=0.01

# Enable Flash Attention for training model optimization (default: 1/true)
# Flash Attention 2 provides 2-4x faster training with no quality loss
# Requires flash-attn package: uv pip install flash-attn --no-build-isolation
# Set to 0 to disable (useful for debugging or older GPUs without Flash Attention support)
GRAIL_TRAINER_USE_FLASH_ATTENTION=1

# Enable gradient checkpointing for memory efficiency (default: 1/true)
# Reduces activation memory by ~20-30% via recomputation on backward pass
# Trade-off: ~10-15% slower training (recomputation cost)
# Benefits: Enables larger effective batch sizes, longer sequences, or training with fewer GPUs
# Set to 0 to disable if you have plenty of memory and want fastest training speed
GRAIL_TRAINER_USE_GRADIENT_CHECKPOINTING=1

# ============================================================================
# GRPO GROUP FILTERING & RANKING CONFIGURATION
# ============================================================================
# Maximum number of groups to use for training (default: 32)
# When more groups are available after filtering, the top-ranked groups by
# combined efficiency score are selected
GRAIL_GRPO_MAX_GROUPS=32

# Maximum completion tokens per rollout (default: 512, 0 disables)
# Filters out groups with any rollout exceeding this length
GRAIL_GRPO_MAX_COMPLETION_TOKENS=512

# Minimum success fraction within a group (default: 0.0, range: 0.0-1.0)
# Filters out groups where fewer than this fraction of rollouts succeeded
# Example: 0.25 requires at least 25% of rollouts in a group to succeed
GRAIL_GRPO_MIN_SUCCESS_FRACTION=0.0

# Minimum mean reward per token threshold (default: 0.0, <=0 disables)
# Filters out groups with reward/token below this threshold
# Implements GFPO (Group Filtered Policy Optimization) efficiency filtering
GRAIL_GRPO_MIN_REWARD_PER_TOKEN=0.0

# Drop lowest quantile by reward/token (default: 0.0, range: 0.0-1.0)
# Drops the bottom X% of groups ranked by reward/token before final selection
# Example: 0.1 drops the worst 10% of groups
GRAIL_GRPO_REWARD_PER_TOKEN_DROP_QUANTILE=0.0

# Group ranking weights for combined efficiency score (must sum to 1.0)
# These weights determine how groups are ranked when selecting top groups:
# - REWARD_WEIGHT: Prioritizes token-efficient solutions (GFPO principle)
# - VARIANCE_WEIGHT: Prioritizes strong learning signal (advantage spread)
# Default: 0.7/0.3 favors efficiency while considering signal strength
GRAIL_GRPO_RANKING_REWARD_WEIGHT=0.7
GRAIL_GRPO_RANKING_VARIANCE_WEIGHT=0.3

# ============================================================================
# GRAIL CHECKPOINT CONFIGURATION
# ============================================================================
# Number of checkpoints to keep on R2 (default: 10)
GRAIL_CHECKPOINT_RETENTION_LIMIT=3

# Interval for milestone checkpoints (every N windows, default: 100)
GRAIL_CHECKPOINT_MILESTONE_INTERVAL=100

# Local cache directory for checkpoints (default: ~/.cache/grail)
GRAIL_CACHE_DIR=

# ============================================================================
# MONITORING SYSTEM CONFIGURATION
# ============================================================================
# Backend type for monitoring ("wandb" for WandB, "null" to disable)
GRAIL_MONITORING_BACKEND=wandb

# ============================================================================
# WANDB (WEIGHTS & BIASES) CONFIGURATION
# ============================================================================
# WandB API key (create at https://wandb.ai/settings)
# Required for WANDB_MODE="online"
WANDB_API_KEY=
# WandB project name (creates a project in your WandB workspace)
WANDB_PROJECT=grail

# WandB entity/team name (your username or team name, optional)
WANDB_ENTITY=tplr

# WandB mode - controls how data is sent to WandB
# - "online": Send data to WandB cloud in real-time (production)
# - "offline": Store data locally, sync later with: wandb sync
# - "disabled": Disable WandB completely
WANDB_MODE=online

# Tags for organizing runs (comma-separated)
WANDB_TAGS=grail,bittensor,production

# Description/notes for runs
WANDB_NOTES=GRAIL production monitoring

# Resume behavior for interrupted runs
# - "allow": Resume if possible, create new otherwise
# - "must": Must resume existing run (fails if not found)
# - "never": Always create new run
# - "auto": Automatically resume based on run ID
WANDB_RESUME=allow

# WandB cache directories (optional - set if local storage is limited)
# Point to a location with more available space to prevent disk full errors
WANDB_CACHE_DIR=
WANDB_DATA_DIR=

# ============================================================================
# MONITORING PERFORMANCE TUNING
# ============================================================================
# Number of metrics to buffer before flushing to WandB (default: 100)
# Higher values = less network calls but more memory usage
GRAIL_METRIC_BUFFER_SIZE=100

# Interval in seconds between automatic metric flushes (default: 30.0)
# Lower values = more real-time data but more network overhead
GRAIL_METRIC_FLUSH_INTERVAL=30.0

# ============================================================================
# NETWORK TIMEOUT AND RETRY CONFIGURATION
# ============================================================================
# Timeout in seconds for Bittensor network calls (default: 15.0)
# Increase if you experience frequent timeouts on slow networks
BT_CALL_TIMEOUT=15.0

# Number of retries for failed Bittensor network calls (default: 3)
# Higher values provide more resilience but may increase latency
BT_CALL_RETRIES=3

# Backoff delay in seconds between retry attempts (default: 5.0)
# Exponential backoff helps avoid overwhelming busy nodes
BT_CALL_BACKOFF=5.0


# ============================================================================
# OBSERVABILITY (Grafana + Loki via Promtail) [Optional]
# ============================================================================
# IMPORTANT: This section is completely optional. Only configure if you want
# centralized logging and monitoring with Grafana + Loki.
#
# Use Promtail to ship logs to Loki for better reliability
# and performance. The app writes logs locally; Promtail tails and forwards them.
#
# SETUP REQUIREMENTS:
# 1. Deploy Grafana + Loki on a separate server using:
#    docker-compose --env-file .env -f docker/compose.grafana.yaml up -d
# 2. Configure PROMTAIL_LOKI_URL to point to your Loki instance
# 3. Set GRAIL_ENV and ensure your wallet/network vars are configured
# 4. Set GRAIL_LOG_FILE to enable file logging for Promtail to tail
#
# Promtail will automatically use labels from your environment:
# - env (from GRAIL_ENV), service=grail, network (from BT_NETWORK)
# - netuid (from NETUID), wallet (from BT_WALLET_COLD), hotkey (from BT_WALLET_HOT)
#
# If you don't want observability, set PROMTAIL_ENABLE=false

# Enable Promtail-based log shipping (required for Grafana logging)
# For docker-compose.validator.yml:
#   - PROMTAIL_ENABLE=false (default): Promtail will NOT start
#   - To enable: docker-compose --profile promtail -f docker/docker-compose.validator.yml up -d
#   - Or set: export COMPOSE_PROFILES=promtail
PROMTAIL_ENABLE=false

# Loki push endpoint URL for Promtail (replace with your Grafana/Loki server)
# Example: http://your-grafana-server.com:3100/loki/api/v1/push
PROMTAIL_LOKI_URL=http://loki:3100/loki/api/v1/push

# Job name for Promtail scraping
PROMTAIL_JOB=grail

# App log file path (required for Promtail tailing)
# Promtail will tail this file and ship logs to Loki
GRAIL_LOG_FILE=/var/log/grail/grail.log

# Optional: log rotation for file logging (used by RotatingFileHandler)
# Accepts bytes or units: KB, MB, GB (e.g., 100MB)
GRAIL_LOG_MAX_SIZE=100MB
# Number of rotated log files to keep
GRAIL_LOG_BACKUP_COUNT=5

# Environment label for promtail logs (dev, staging, prod, etc.)
GRAIL_ENV=prod

# Grafana server root URL (used when deploying Grafana with compose.grafana.yaml)
# This should match the public URL where your Grafana instance will be accessible
# Default: http://localhost:3000 (if not set)
# GF_SERVER_ROOT_URL=http://your-grafana-server.com:3000
